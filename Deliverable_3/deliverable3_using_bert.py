# -*- coding: utf-8 -*-
"""Deliverable3_using_bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QXS49geWxTx-vpehBQmmrtXRkIBrRsg0

## Algorithms for Data Science - CS 676.
## Project 1 - Deliverable 3.
## Ali Suhel Inamdar.

## USING BERT TOKENIZER.

**- Installing the Hugging Face Update Package.**
"""

#Installing the hugging face update package
! pip install --upgrade huggingface_hub

!pip install -U sentence-transformers

"""## 1. Combining the CSV."""

#Installing the libararies

import pandas as pd
import requests
from io import StringIO

#Creating a function to download and combine all csv from github.

def download_and_combine_csv(urls: list) -> pd.DataFrame:
    """
    Download CSV files from provided URLs and combine them row-wise.
    It will combine files if they have the expected column names or just the correct number of columns.
    Handles different encodings to avoid common decoding errors. Converts certain columns to integer form.

    Args:
    urls (list): List of strings containing the URLs of the CSV files.

    Returns:
    pd.DataFrame: A DataFrame containing the combined data from all matching CSV files.
    """
    expected_columns = {'user_prompt', 'url_to_check', 'func_rating', 'custom_rating'}
    combined_df = pd.DataFrame()

    for url in urls:
        try:
            # Modify the GitHub URL to point to the raw version of the file
            raw_url = url.replace('github.com', 'raw.githubusercontent.com').replace('/blob', '')

            response = requests.get(raw_url)
            if response.status_code == 200:
                try:
                    # Attempt to read with utf-8 encoding
                    content = response.content.decode('utf-8')
                except UnicodeDecodeError:
                    # Attempt to read with latin1 encoding
                    content = response.content.decode('latin1')

                # Read the CSV file into a DataFrame
                df = pd.read_csv(StringIO(content))

                # Check if the columns match the expected columns or if there are at least 4 columns
                if set(df.columns) == expected_columns:
                    combined_df = pd.concat([combined_df, df], ignore_index=True)
                elif len(df.columns) >= 4:
                    # Assume the first 4 columns are the required ones and ignore the original column names
                    df = df.iloc[:, :4]
                    df.columns = ['user_prompt', 'url_to_check', 'func_rating', 'custom_rating']
                    combined_df = pd.concat([combined_df, df], ignore_index=True)
                else:
                    print(f"Skipping file with insufficient columns: {url}")
            else:
                print(f"Failed to download from {url}")
        except Exception as e:
            print(f"Error processing {url}: {str(e)}")

    # Ensure func_rating and custom_rating are integers
    if not combined_df.empty:
        combined_df['func_rating'] = combined_df['func_rating'].round().astype(int)
        combined_df['custom_rating'] = combined_df['custom_rating'].round().astype(int)

    return combined_df

#Getting all csv from github links
urls = [
    "https://github.com/vrutika-prajapati/Credibility-Score-for-Articles/blob/main/projects/deliverable%202/Url_validation.csv",
    "https://github.com/Saikumar08-sk/URL-Validation/blob/main/Deliverable.csv",
    "https://github.com/aditya19111/Project-1-Credibility-Score-for-Articles-Sources-References/blob/main/Deliverable_2/Deliverables_rating_comparison%20-%20Sheet1.csv",
    "https://github.com/SAIKUMAR500/algorithums-for-data-science/blob/main/deliverable2/dataset(Sheet1).csv",
    "https://github.com/drashti-sanghani/Credibility_score_articles/blob/main/Project/Deliverable2/user_ratings.csv",
    "https://github.com/anchalrai101/CREDIBILITY-SCORE/blob/main/urltesting_scores.csv",
    "https://github.com/AliSInamdar/CS676-Ali-Inamdar/blob/Project_1_Credibilty_Score/Deliverable_2/Deliverable_2.csv",
    "https://github.com/bhavnaa22/Algorithms-For-Data-Science/blob/main/Deliverable%202/Deliverable2.csv",
    "https://github.com/bhatthardik4/AlgorithmDS/blob/main/Deliverable_2/HardikBhattD2.csv",
    "https://github.com/Rakeshkanche/Algorithms-for-Data-Science/blob/main/queries_urls_ratings.csv",
    "https://github.com/kristinakim-code/Credibility-Check/blob/main/deliverable2.csv",
    "https://github.com/bhavnaa22/Algorithms-For-Data-Science/blob/main/Deliverable%202/Deliverable2.csv",
    "https://github.com/kamaldomandula/Algorithms-of-data-science-Project/blob/main/Project/project1/Deliverable2/deliverable.csv",
    "https://github.com/ChinmayShetye26/Algo-for-DS-delivery2/blob/main/Sample.csv",
    "https://github.com/krishnam229/Project1/blob/main/deliverable2/sample.csv",
    "https://github.com/drona23/Deliverable2/blob/main/output.csv",
    "https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/testing/sample.csv"
]

#Coverting the new csv into a data frame.
result_df = download_and_combine_csv(urls)
print(result_df)

#Printing the new data frame.
result_df

"""## 2. Creating Neural Network Models.

- Importing all libraries required for this deliverable.
"""

#impoting Libraries:
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd

"""- Creating a Funtion to build Neural Networks."""

#Importing Tensor Flow libraries
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten

# Define function to create the neural network model
def create_nn_model(vocab_size: int, embedding_dim: int, max_length: int, num_of_dense: int, output_dim: int) -> Model:
    """
    Creates a neural network model that processes user prompts using an embedding layer,
    concatenates it with function ratings, and passes through dense layers.

    Args:
        vocab_size (int): Size of the vocabulary for embedding.
        embedding_dim (int): Dimensionality of the embedding layer.
        max_length (int): Maximum length of input sequences.
        num_of_dense (int): Number of dense layers before concatenation.

    Returns:
        Model: A compiled TensorFlow model.
    """
    # Text input (user prompt)
    text_input = Input(shape=(max_length,), name="text_input")
    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(text_input)
    flatten = Flatten()(embedding)

    # Dense layers for text input
    num_neurons = 2**12  # Start with 4096 neurons
    x = flatten
    for _ in range(num_of_dense):
        num_neurons = max(1, int(num_neurons / 2))  # Ensure integer neurons, minimum of 1
        x = Dense(num_neurons, activation='relu')(x)

    # Numeric input (func_rating)
    func_rating_input = Input(shape=(1,), name="func_rating_input")
    y = Dense(32, activation='relu')(func_rating_input)

    # Concatenate both paths
    concatenated = Concatenate()([x, y])
    # output = Dense(1, activation='linear', name="output")(concatenated)
    output = Dense(output_dim, activation='softmax', name="output")(concatenated)

    # Define and compile the model
    model = Model(inputs=[text_input, func_rating_input], outputs=output)

    # Set up learning rate scheduler
    # code here:

    # Compile
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

#Printing the unique values
result_df["custom_rating"].unique()

#Printing the shape
result_df.shape

"""## 3. Tokenizer to Process the text."""

# Tokenize and prepare data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(result_df["user_prompt"])
vocab_size = len(tokenizer.word_index) + 1
max_length = max([len(x.split()) for x in result_df["user_prompt"]])
embedding_dim = 8

# Convert text data into sequences
X_text = tokenizer.texts_to_sequences(result_df["user_prompt"])
X_text = pad_sequences(X_text, maxlen=max_length, padding='post')
print(X_text.shape)

# Numeric input
X_func_rating = np.array(result_df["func_rating"]).reshape(-1, 1)
print(X_func_rating.shape)

# Target variable
y = np.array(result_df["custom_rating"]).reshape(-1, 1)
print(y.shape)

from transformers import BertTokenizer, TFBertModel
import tensorflow as tf

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize and prepare data for BERT
def prepare_data(texts):
    """
    Prepares input text data into tokenized tensors for BERT.

    Args:
    texts (pd.Series): A pandas series containing text entries.

    Returns:
    tf.Tensor: Tokenized and formatted input for BERT.
    """
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=512, # Can adjust depending on your specific needs
            truncation=True,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='tf',
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])

    input_ids = tf.concat(input_ids, axis=0)
    attention_masks = tf.concat(attention_masks, axis=0)

    return input_ids, attention_masks

# Example usage
X_text, X_mask = prepare_data(result_df["user_prompt"])

print(X_text.shape)  # This will show the shape of your input_ids
print(X_mask.shape)  # This will show the shape of your attention masks

X_mask

result_df["custom_rating"].unique()

"""- Converting into categorical."""

from keras.utils import to_categorical

# Assuming y is your array of class labels shaped as (20, 1)
# Convert labels to one-hot encoding
y_one_hot = to_categorical(y > 3)

# Check the new shape of y_one_hot
print(y_one_hot.shape)

"""## 4. Instantiating the Model"""

# Creating the model
num_of_dense_layers = 2  # Example: 2 dense layers before concatenation
model = create_nn_model(vocab_size, embedding_dim, max_length, num_of_dense_layers, 2)

# Example of setting parameters based on your dataset
vocab_size = 30  # Example for BERT-base which has a vocab size of about 30,522
embedding_dim = 100  # Example dimensionality
max_length = 512  # From your output, each input sequence length
num_of_dense = 10  # Example of using three dense layers
output_dim = 2  # Number of output classes, change this as per your specific task

# Initialize the model with the parameters
model = create_nn_model(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length, num_of_dense=num_of_dense, output_dim=output_dim)

"""## Training the model."""

# Example of setting parameters based on your dataset
# vocab_size = 30  # Example for BERT-base which has a vocab size of about 30,522 - THIS IS INCORRECT
# vocab_size = len(tokenizer.word_index) + 1 # Use the vocab_size calculated earlier during tokenization - This line is causing the error
vocab_size = tokenizer.vocab_size # BertTokenizer uses vocab_size attribute instead of word_index
embedding_dim = 100  # Example dimensionality
max_length = 512  # From your output, each input sequence length
num_of_dense = 10  # Example of using three dense layers
output_dim = 2  # Number of output classes, change this as per your specific task

# Initialize the model with the parameters
model = create_nn_model(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length, num_of_dense=num_of_dense, output_dim=output_dim)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Train the model
# model.fit(
#     {"text_input": X_text, "func_rating_input": X_func_rating},
#     y_one_hot,
#     epochs=100,
#     batch_size=5,
#     validation_split=0.05,
#     verbose=2
# )

# Plot error
import matplotlib.pyplot as plt

plt.plot(model.history.history['loss'])
plt.plot(model.history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

# Display the model summary
model.summary()

from sklearn.metrics import classification_report
import numpy as np

# Predict probabilities for the test set
# Assume X_test_text and X_test_func_rating are your test datasets prepared similarly to your training datasets
predictions = model.predict({"text_input": X_text, "func_rating_input": X_func_rating})

# Convert probabilities to class labels
predicted_classes = np.argmax(predictions, axis=1)

# Since y_test is in one-hot encoded format, convert it back to class labels for evaluation
true_classes = np.argmax(y_one_hot, axis=1)

# Generate a classification report
report = classification_report(true_classes, predicted_classes)
print(report)

"""- Creating Repo on HF"""

from google.colab import userdata
userdata.get('HuggingFaceToken')

from huggingface_hub import HfApi

hf_username = "AliInamdar"  # Replace with your Hugging Face username
repo_name = "deliverable3_with-bert"  # Change this as needed
repo_id = f"{hf_username}/{repo_name}"

api = HfApi()

# Create the repository
api.create_repo(repo_id=repo_id, exist_ok=True)  # exist_ok=True prevents errors if the repo already exists

print(f"Repository created: https://huggingface.co/{repo_id}")

"""- Pushing Artifacts on HF"""

import pickle
import os
from huggingface_hub import HfApi
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer

# Define save directory
model_dir = "./saved_model"
os.makedirs(model_dir, exist_ok=True)

# Save the model
model_path = os.path.join(model_dir, "model.keras")
model.save(model_path)

# Save the tokenizer as a pickle file
tokenizer_path = os.path.join(model_dir, "tokenizer.pkl")
with open(tokenizer_path, "wb") as f:
    pickle.dump(tokenizer, f)

# Upload model and tokenizer to Hugging Face
repo_id = f"AliInamdar/{repo_name}"  # Change as needed
api = HfApi()

api.upload_file(path_or_fileobj=model_path, path_in_repo="model.keras", repo_id=repo_id)
api.upload_file(path_or_fileobj=tokenizer_path, path_in_repo="tokenizer.pkl", repo_id=repo_id)

print(f"Model and tokenizer successfully pushed to: https://huggingface.co/{repo_id}")

"""- Download Model Artifact from HF"""

import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from huggingface_hub import hf_hub_download

# Set Keras backend to JAX (Optional, only needed if you want to experiment with JAX backend)
os.environ["KERAS_BACKEND"] = "jax"

# Define repo ID and model filename
filename = "model.keras"  # Ensure this matches what was uploaded

# Download the model
model_path = hf_hub_download(repo_id=repo_id, filename=filename)

# Load the Keras model
new_model = keras.models.load_model(model_path)

print("Model loaded successfully!")

new_model.summary()

"""- Local Inference"""

#Running the local Inference of the model
import pickle
from huggingface_hub import hf_hub_download
from tensorflow import keras
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Define repo details
# repo_id = "eagle0504/my-tf-nn-model" # we already defined above

# Download model
# model_path = hf_hub_download(repo_id=repo_id, filename="model.keras")
# new_model = keras.models.load_model(model_path)

# Download tokenizer
tokenizer_path = hf_hub_download(repo_id=repo_id, filename="tokenizer.pkl")

# Load the tokenizer
with open(tokenizer_path, "rb") as f:
    tokenizer = pickle.load(f)

print("Model and tokenizer loaded successfully!")

# Sample test data
test_texts = [
    "How to improve focus and concentration?",
    "What are the side effects of lack of sleep?",
]

# Preprocess test data using the loaded tokenizer
max_length = model.input_shape[0][1]  # Get max_length from model input shape
X_text_test = tokenizer.texts_to_sequences(test_texts)
X_text_test = pad_sequences(X_text_test, maxlen=max_length, padding='post')

# Dummy numeric input (func_rating)
X_func_test = np.array([5, 4]).reshape(-1, 1)

# Make predictions
predictions = new_model.predict({"text_input": X_text_test, "func_rating_input": X_func_test})

# Display results
for i, text in enumerate(test_texts):
    print(f"Prompt: {text}")
    print(f"Predicted Rating: {predictions[i][0]:.2f}")
    print("-" * 50)